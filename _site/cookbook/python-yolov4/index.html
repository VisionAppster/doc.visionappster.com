
<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Yolo V4 object detection with OpenCV and Python &mdash; VisionAppster  documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/docsearch.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/v.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script defer="defer" src="../../_static/js/docsearch.min.js"></script>
        <script defer="defer" src="../../_static/js/algolia.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex/" />
    <link rel="search" title="Search" href="../../search/" />
    <link rel="next" title="Positioning with QR codes" href="../qr-positioning/" />
    <link rel="prev" title="Super-resolution with OpenCV and Python" href="../python-superresolution/" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <div data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search" >
        

        
          <a href="../../">
        

        
          
          <img src="../../_static/visionappster.png" class="logo" alt="Logo">
        
        </a>

        
          
          
        

        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search/" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

        
      </div>

      <nav class="wy-nav-side-inner">
        <div class="wy-side-scroll">
          
          <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
              
              
                
              
              
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../engine/">Engine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../builder/">Builder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tools/">Built-in tools</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/">APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../components/">Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cameras/">Cameras</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../">Cookbook</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../analyze-blob-geometry/">Analyze Blob Geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../binarize/">Binarize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../blurring/">Blurring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../caliper/">Caliper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../code-reading/">Code reading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contour-matching/">Contour matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../detect-blobs/">Detect Blobs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../image-classifier/">Image classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="../manual-calibration/">Manual Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../motion-detector/">Motion detector</a></li>
<li class="toctree-l2"><a class="reference internal" href="../opencv-tools/">OpenCV Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../python-superresolution/">Super-resolution with OpenCV and Python</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Yolo V4 object detection with OpenCV and Python</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#summary">Summary</a></li>
<li class="toctree-l3"><a class="reference internal" href="#downloads">Downloads</a></li>
<li class="toctree-l3"><a class="reference internal" href="#detailed-description">Detailed description</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#setting-up-the-environment">Setting up the environment</a></li>
<li class="toctree-l4"><a class="reference internal" href="#python-code-walk-through">Python code walk-through</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-the-tool-in-the-builder">Using the tool in the Builder</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../qr-positioning/">Positioning with QR codes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../raspi-code-reading/">Code reading for Raspberry Pi</a></li>
<li class="toctree-l2"><a class="reference internal" href="../texture-matching/">Texture matching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../unsharp-masking/">Unsharp Masking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../web-app/">Web app</a></li>
<li class="toctree-l2"><a class="reference internal" href="../white-balance/">White Balance Correction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../yolo-classifier/">YOLO classifier</a></li>
</ul>
</li>
</ul>

              
            
          </div>
          
        </div>
      </nav>
    </div>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../">VisionAppster</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content style-external-links">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../">Cookbook</a> &raquo;</li>
        
      <li>Yolo V4 object detection with OpenCV and Python</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="yolo-v4-object-detection-with-opencv-and-python">
<h1>Yolo V4 object detection with OpenCV and Python<a class="headerlink" href="#yolo-v4-object-detection-with-opencv-and-python" title="Permalink to this headline">🔗</a></h1>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">🔗</a></h2>
<p>We’ll write a few lines of Python code that uses OpenCV’s neural network
module to implement a Yolo V4 object detector. The tool gives the
locations and names of up to 80 kinds of different objects in input
images.</p>
</div>
<div class="section" id="downloads">
<h2>Downloads<a class="headerlink" href="#downloads" title="Permalink to this headline">🔗</a></h2>
<ul class="simple">
<li><p><a class="reference download internal" download="" href="../../_downloads/2e215e414c157a3e78e89b592b355fb4/yolo_toolplugin.py"><code class="xref download docutils literal notranslate"><span class="pre">Python</span> <span class="pre">tool</span> <span class="pre">plugin</span></code></a></p></li>
<li><p><a class="reference external" href="https://github.com/AlexeyAB/darknet/blob/master/cfg/coco.names">Class
names</a></p></li>
<li><p><a class="reference external" href="https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4.cfg">Yolo V4 configuration
file</a></p></li>
<li><p><a class="reference external" href="https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights">Yolo V4 weights (258
MB)</a></p></li>
</ul>
</div>
<div class="section" id="detailed-description">
<h2>Detailed description<a class="headerlink" href="#detailed-description" title="Permalink to this headline">🔗</a></h2>
<div class="section" id="setting-up-the-environment">
<h3>Setting up the environment<a class="headerlink" href="#setting-up-the-environment" title="Permalink to this headline">🔗</a></h3>
<p>As explained in <a class="reference internal" href="../../api/python/tool/"><span class="doc">Python tool API</span></a>, the Builder will
execute all Python modules whose name ends with <code class="docutils literal notranslate"><span class="pre">*toolplugin.py</span></code> in
<code class="docutils literal notranslate"><span class="pre">$HOME/VisionAppster/plugins/tool/</span></code>. If these modules register tools,
they will appear in Builder’s tool box. In this example, the Python
module will be <a class="reference download internal" download="" href="../../_downloads/2e215e414c157a3e78e89b592b355fb4/yolo_toolplugin.py"><code class="xref download docutils literal notranslate"><span class="pre">yolo_toolplugin.py</span></code></a> and
the name of the tool in the plugin is <code class="docutils literal notranslate"><span class="pre">Yolo</span></code>.</p>
<p>The Yolo tool needs three files that contain class names
(<code class="docutils literal notranslate"><span class="pre">coco.names</span></code>), Yolo V4 configuration (<code class="docutils literal notranslate"><span class="pre">yolov4.cfg</span></code>) and the weights
of the neural network (<code class="docutils literal notranslate"><span class="pre">yolov4.weights</span></code>). To set up the environment,
download the files and place them in
<code class="docutils literal notranslate"><span class="pre">$HOME/VisionAppster/plugins/tool/yolov4</span></code> as shown below.</p>
<div class="medium figure align-default" id="id1">
<img alt="Downloaded files" src="../../_images/yolov4-1.png" />
<p class="caption"><span class="caption-text">Downloaded files</span><a class="headerlink" href="#id1" title="Permalink to this image">🔗</a></p>
</div>
<p>Use the <a class="reference internal" href="../../components/installing/"><span class="doc">va-pkg command-line tool</span></a> to enable
Python support and to install the required NumPy and OpenCV packages:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>va-pkg install com.visionappster.extensions.python
va-pkg install python:numpy python:opencv-python
</pre></div>
</div>
</div>
<div class="section" id="python-code-walk-through">
<h3>Python code walk-through<a class="headerlink" href="#python-code-walk-through" title="Permalink to this headline">🔗</a></h3>
<p>Let us walk through <a class="reference download internal" download="" href="../../_downloads/2e215e414c157a3e78e89b592b355fb4/yolo_toolplugin.py"><code class="xref download docutils literal notranslate"><span class="pre">the</span> <span class="pre">Python</span> <span class="pre">code</span></code></a>.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">visionappster</span> <span class="k">as</span> <span class="nn">va</span>
<span class="kn">from</span> <span class="nn">visionappster.coordinates</span> <span class="kn">import</span> <span class="n">pixel_to_world</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">os</span>
</pre></div>
</div>
<p>Import the necessary modules. <code class="docutils literal notranslate"><span class="pre">visionappster</span></code> is needed in every
Python tool, <code class="docutils literal notranslate"><span class="pre">numpy</span></code> almost as often. <code class="docutils literal notranslate"><span class="pre">cv2</span></code> is the name of the
OpenCV module. <code class="docutils literal notranslate"><span class="pre">os</span></code> is needed for path manipulation.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Yolo</span><span class="p">:</span>
    <span class="c1"># Shared by all instances of this class</span>
    <span class="n">_dnn_model</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">_class_names</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_load_yolo_model</span><span class="p">():</span>
        <span class="c1"># Initialize the DNN model</span>
        <span class="n">data_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;/yolov4/&#39;</span>
        <span class="n">Yolo</span><span class="o">.</span><span class="n">_dnn_model</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">dnn_DetectionModel</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">+</span> <span class="s1">&#39;yolov4.cfg&#39;</span><span class="p">,</span> <span class="n">data_dir</span> <span class="o">+</span> <span class="s1">&#39;yolov4.weights&#39;</span><span class="p">)</span>
        <span class="n">Yolo</span><span class="o">.</span><span class="n">_dnn_model</span><span class="o">.</span><span class="n">setInputSize</span><span class="p">(</span><span class="mi">704</span><span class="p">,</span> <span class="mi">704</span><span class="p">)</span>
        <span class="n">Yolo</span><span class="o">.</span><span class="n">_dnn_model</span><span class="o">.</span><span class="n">setInputScale</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span>
        <span class="n">Yolo</span><span class="o">.</span><span class="n">_dnn_model</span><span class="o">.</span><span class="n">setInputSwapRB</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">data_dir</span> <span class="o">+</span> <span class="s1">&#39;coco.names&#39;</span><span class="p">,</span> <span class="s1">&#39;rt&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">Yolo</span><span class="o">.</span><span class="n">_class_names</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">Yolo</span><span class="o">.</span><span class="n">_dnn_model</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">Yolo</span><span class="o">.</span><span class="n">_load_yolo_model</span><span class="p">()</span>
</pre></div>
</div>
<p>The size of the neural network (NN) model is hundreds of megabytes.
Fortunately, it isn’t modified at run time and can thus be shared
between all instance of the tool class. This piece of code reads the NN
configuration, weights and class names from files on the disk. Only the
first instance of the Yolo class creates the model.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
            <span class="n">inputs</span><span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;image&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">.</span><span class="n">Image</span><span class="p">,</span> <span class="n">va</span><span class="o">.</span><span class="n">Image</span><span class="p">()),</span>
                     <span class="p">(</span><span class="s1">&#39;confidenceThreshold&#39;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;min&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">}),</span>
                     <span class="p">(</span><span class="s1">&#39;suppressionThreshold&#39;</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;min&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">})],</span>
            <span class="n">outputs</span><span class="p">:</span> <span class="p">[(</span><span class="s1">&#39;className&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">.</span><span class="n">Array</span><span class="p">),</span>
                      <span class="p">(</span><span class="s1">&#39;classId&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Int32</span><span class="p">),</span>
                      <span class="p">(</span><span class="s1">&#39;confidence&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Double</span><span class="p">),</span>
                      <span class="p">(</span><span class="s1">&#39;frame&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Double</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;typeName&#39;</span><span class="p">:</span> <span class="s1">&#39;Matrix&lt;double&gt;/frame&#39;</span><span class="p">,</span>
                                                   <span class="s1">&#39;blockSize&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">}),</span>
                      <span class="p">(</span><span class="s1">&#39;size&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Double</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;typeName&#39;</span><span class="p">:</span> <span class="s1">&#39;Matrix&lt;double&gt;/size&#39;</span><span class="p">,</span>
                                                  <span class="s1">&#39;linkedTo&#39;</span><span class="p">:</span> <span class="s1">&#39;frame/region&#39;</span><span class="p">,</span>
                                                  <span class="s1">&#39;blockSize&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}),</span>
                      <span class="p">(</span><span class="s1">&#39;numberOfObjects&#39;</span><span class="p">,</span> <span class="nb">int</span><span class="p">)]):</span>
</pre></div>
</div>
<p>This is the declaration of the tool’s external interface. There are
three inputs to the tool</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">image</span></code> is the input image of type <code class="docutils literal notranslate"><span class="pre">va.Image</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">confidenceThreshold</span></code> determines the minimum confidence of an
accepted detection. The bigger the value, the fewer false positives
are expected. The value range is 0…1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">suppressionThreshold</span></code> determines how eagerly close-by objects are
grouped into one detection. The bigger the value, the more likely it
is that two objects are grouped as one. Conversely, the smaller the
value, the more likely one object will be detected as two or more
separate objects. The value range is 0…1.</p></li>
</ul>
<p>The six outputs are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">className</span></code> is an array containing the names of the detected
objects.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">classId</span></code> is an N-by-1 matrix of class identifiers (i.e. integers
on range 0…79).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">confidence</span></code> is an N-by-1 matrix storing the likelihood of correct
detection for each detected object.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">frame</span></code> is a 4N-by-4 matrix containing a coordinate frame for the
location of each detection. The <code class="docutils literal notranslate"><span class="pre">blockSize</span></code> meta-data gives the
Builder a hint that there may be many frames that should all be
displayed individually.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">size</span></code> is an N-by-2 matrix storing the size of each detection in
the world coordinate system. The <code class="docutils literal notranslate"><span class="pre">linkedTo</span></code> meta-data hints the
Builder that the size should be interpreted relative to the <code class="docutils literal notranslate"><span class="pre">frame</span></code>
output parameter and that it is best displayed as a region.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">numberOfObjects</span></code> is the number of objects that were detected.</p></li>
</ul>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">inputs</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">is_empty</span><span class="p">():</span>
    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Input image must be non-empty.&#39;</span><span class="p">)</span>

<span class="c1"># The neural network requires 704x704x3 input.</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">image</span><span class="o">.</span><span class="n">to_rgb</span><span class="p">()</span><span class="o">.</span><span class="n">scaled</span><span class="p">(</span><span class="mi">704</span><span class="p">,</span> <span class="mi">704</span><span class="p">)</span>
</pre></div>
</div>
<p>Make sure that the input can be scaled to the size required by the NN
model and then scale it. The input to the Yolo V4 network is a tensor
whose shape is (704, 704, 3). That is a fancy way to describe a
704-by-704 RGB color image.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># Do the detection using the given confidence + non-maximum</span>
<span class="c1"># suppression thresholds</span>
<span class="n">classes</span><span class="p">,</span> <span class="n">confidences</span><span class="p">,</span> <span class="n">boxes</span> <span class="o">=</span>
    <span class="n">Yolo</span><span class="o">.</span><span class="n">_dnn_model</span><span class="o">.</span><span class="n">detect</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span>
                           <span class="n">confThreshold</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">confidenceThreshold</span><span class="p">,</span>
                           <span class="n">nmsThreshold</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">suppressionThreshold</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">detect</span></code> function takes a NumPy array as an input. Luckily,
converting a <code class="docutils literal notranslate"><span class="pre">va.Image</span></code> to such an array is easy. With <code class="docutils literal notranslate"><span class="pre">copy=False</span></code>,
the array will be a shallow copy that points to the data buffer of
<code class="docutils literal notranslate"><span class="pre">img</span></code>.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">class_names</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">num_objects</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">confidences</span><span class="p">)</span>
<span class="k">if</span> <span class="n">num_objects</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">confidences</span> <span class="o">=</span> <span class="n">confidences</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="n">classes</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">class_id</span> <span class="ow">in</span> <span class="n">classes</span><span class="p">:</span>
        <span class="n">class_names</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Yolo</span><span class="o">.</span><span class="n">_class_names</span><span class="p">[</span><span class="n">class_id</span><span class="p">])</span>

<span class="n">outputs</span><span class="o">.</span><span class="n">className</span> <span class="o">=</span> <span class="n">class_names</span>
<span class="n">outputs</span><span class="o">.</span><span class="n">numberOfObjects</span> <span class="o">=</span> <span class="n">num_objects</span>
</pre></div>
</div>
<p>Map the class indices given by the NN model to class names according to
the static <code class="docutils literal notranslate"><span class="pre">class_names</span></code> array. We can now set two of the outputs.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create output matrices</span>
<span class="n">outputs</span><span class="o">.</span><span class="n">confidence</span> <span class="o">=</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Double</span><span class="p">(</span><span class="n">num_objects</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">outputs</span><span class="o">.</span><span class="n">classId</span> <span class="o">=</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Int32</span><span class="p">(</span><span class="n">num_objects</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">outputs</span><span class="o">.</span><span class="n">frame</span> <span class="o">=</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Double</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">num_objects</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">outputs</span><span class="o">.</span><span class="n">size</span> <span class="o">=</span> <span class="n">va</span><span class="o">.</span><span class="n">Matrix</span><span class="o">.</span><span class="n">Double</span><span class="p">(</span><span class="n">num_objects</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Copy results to output matrices.</span>
<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_objects</span><span class="p">):</span>
    <span class="c1"># Coordinate frames are aligned to image axes.</span>
    <span class="c1"># Initialize to identity matrices.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">outputs</span><span class="o">.</span><span class="n">frame</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">j</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">confidence</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">confidences</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">classId</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">classes</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">left</span><span class="p">,</span> <span class="n">top</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="n">boxes</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="c1"># Move corner points to world coordinates</span>
    <span class="n">x0</span><span class="p">,</span> <span class="n">y0</span> <span class="o">=</span> <span class="n">pixel_to_world</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">left</span><span class="p">,</span> <span class="n">top</span><span class="p">)</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">pixel_to_world</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">left</span> <span class="o">+</span> <span class="n">width</span><span class="p">,</span> <span class="n">top</span> <span class="o">+</span> <span class="n">height</span><span class="p">)</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">frame</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">j</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">frame</span><span class="p">[</span><span class="mi">4</span> <span class="o">*</span> <span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">y0</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x1</span> <span class="o">-</span> <span class="n">x0</span>
    <span class="n">outputs</span><span class="o">.</span><span class="n">size</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">y1</span> <span class="o">-</span> <span class="n">y0</span>
</pre></div>
</div>
<p>This piece of code first creates empty matrices to hold the confidence,
class ID and location of each detection result. The loop then copies the
data from the output tensors of the NN model to the matrices.</p>
<p>An important thing to note here is the mapping from image coordinates to
<a class="reference internal" href="../../engine/world-coordinates/"><span class="doc">world coordinates</span></a>. This lets subsequent
analysis tools and the user interface to relate the output coordinates
to the input image (even though the input to the NN was scaled) and
eventually to the real world if the image originated from a camera.</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="n">va</span><span class="o">.</span><span class="n">Tool</span><span class="o">.</span><span class="n">publish</span><span class="p">(</span><span class="s1">&#39;com.visionappster.opencvpython/1&#39;</span><span class="p">,</span> <span class="n">Yolo</span><span class="p">)</span>
</pre></div>
</div>
<p>This publishes the tool to the VisionAppster runtime. The unique
component ID of tools in this Python plugin is
<code class="docutils literal notranslate"><span class="pre">com.visionappster.opencvpython/1</span></code>, where “1” denotes a major version
number.</p>
</div>
<div class="section" id="using-the-tool-in-the-builder">
<h3>Using the tool in the Builder<a class="headerlink" href="#using-the-tool-in-the-builder" title="Permalink to this headline">🔗</a></h3>
<p>Once you save the Python file in
<code class="docutils literal notranslate"><span class="pre">$HOME/VisionAppster/plugins/tool/yolo_toolplugin.py</span></code> and start the
Builder (or click the “Refresh user plugins” button if the Builder is
already running), you’ll see the new tool in a group labeled
OPENCVPYTHON in the Builder’s tool box. (Make sure to save the NN data
files as well.)</p>
<p>Drag and drop the Yolo tool from the tool box on the workspace. Drag and
drop the <code class="docutils literal notranslate"><span class="pre">image</span></code> input of the tool on the workspace to open an image
selector and pick an input image. (Double-click to select.) Then start
the app using the play (▶) button on the status bar.</p>
<p>From the window menu of the image display, select Display with → Image
display. Then drag and drop <code class="docutils literal notranslate"><span class="pre">frame</span></code> output of the Yolo tool on the
image display to see the bounding boxes of the detected objects. Drag
and drop the <code class="docutils literal notranslate"><span class="pre">className</span></code> output on the workspace as well. Here is what
you may get:</p>
<div class="figure align-default" id="id2">
<img alt="Yolo seems to know what a dog is." src="../../_images/yolov4-2.png" />
<p class="caption"><span class="caption-text">Yolo seems to know what a dog is.</span><a class="headerlink" href="#id2" title="Permalink to this image">🔗</a></p>
</div>
<p>To see why the coordinates were converted using <code class="docutils literal notranslate"><span class="pre">pixel_to_world</span></code> you
can create the processing graph shown below. It uses the <code class="docutils literal notranslate"><span class="pre">Rescale</span></code>
tool from <a class="reference internal" href="../python-superresolution/"><span class="doc">another Python cookbook
entry</span></a> to upscale the input image by
a factor of two. Since this tool outputs the detection locations in the
world coordinate system and the <code class="docutils literal notranslate"><span class="pre">Rescale</span></code> tool outputs an image with a
world coordinate system attached, you can drag and drop the <code class="docutils literal notranslate"><span class="pre">frame</span></code>
and <code class="docutils literal notranslate"><span class="pre">size</span></code> output parameters of this tool on top of it and see the
regions in the expected locations.</p>
<div class="figure align-default" id="id3">
<img alt="World coordinates make measurements easier." src="../../_images/yolov4-3.png" />
<p class="caption"><span class="caption-text">World coordinates make measurements easier.</span><a class="headerlink" href="#id3" title="Permalink to this image">🔗</a></p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../qr-positioning/" class="btn btn-neutral float-right" title="Positioning with QR codes" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../python-superresolution/" class="btn btn-neutral float-left" title="Super-resolution with OpenCV and Python" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, VisionAppster.

    </p>
  </div> 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-129514554-1', 'auto');
    
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>